---
title: "Problem Intro and simulation setting"
author: "yuqimiao"
date: "2020-09-08"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r}
library(SNFtool)
library(rlist)
library(tidyverse)
library(abSNF)
library(SIMLR)
library(parallel)
library(Matrix)
source("./code/R/compute.multiple.kernel.R")
source("./code/functions/simulation_function.R")
```

# Goal of study

We want to find a data integration method which can accurately identify the subtypes embeded in multi-omics data of patients.

## Problems want to solve in this method

1. Find the structure of different omics data, and compare similarity/difference;
2. Integrate complementary information from multi-omics data to capture as much as subtype information as possible;
3. Avoid influence from noise features.

# simulation logic

## Mechanism and characteristics parameters

The goal of simulation is to embed differences in the signal features of different subtypes. Although for the same set of samples, they could imply different cluster information, and the difference can be shown from the effective size, the noise-to-signal ratio, the cluster type seperation and the feature distribution type.

Here we have 2 different settings to imply 2 scenarios:

Assuming 200 samples in 4 different subtypes, overall 4 data types. Every data type has 1000 features, with different noise-signal ratio and feature variance. The signal features are coming from different distributions with different parameters for different subtypes, which is the mechanism to generate subtype difference.

More specifically, let's assume the first data type (n = 200 * p = 1000) can separate 4 subtypes with balanced subtype ratio(50 subjects per subtype). The first 50 features are the signals which actually effective in subtype separation. For the first subtype, the 50 feature signal are generated from $N(\mu_1, \sigma^2)$, the second subtype, $N(\mu_2, \sigma^2)$ and so on, thus, the difference between $\mu_i$ and the size of $\sigma$ will influence the degree of separation of 4 subtypes. Also, not all features would follow a normal distribution. For example, beta measurement of methylation rates for a CpG sites could follow a beta distrbution.

## [NEW] Information possession setting 

Besides the subtype ratio, $\mu_i$ and $\sigma$ and distribution type, there is another factor could influence the result of integration. Different data types could include different separation information. For example, subtype A and B have analygous gene expression profile, and subtype C and D also have analygous gene expression profile, but the subtype combinations A&B and C&D may have different gene expression profile. In this setting, using gene expression data only, we can separate A&B and C&D as 2 subtypes. Similarly, methylation data can separate A&C and B&D as 2 subtypes. In this setting, if we can properly integrate these 2 data types, we can have 4 subtype groups.But there are many possibilities for the information possession setting for each data type. Here we only consider 2 easy scenario.

1) different types of data imply same cluster information

Here we assume all data types reflect same clustering information, which means all data types can separate 4 different subtypes with similar clustering analogy. We can assume for data type m, the signal features for 4 subtypes comes from $N(\mu_{im}, \sigma^2), i = 1,2,3,4$ separately. We set the parameter: $\{\mu_{im}\} = \{-\mu_m,\mu_m,-2\mu_m,2\mu_m\}$

```{r}
source("./code/functions/simulation_function.R")
sim1 = simulation_3(data_divide = rep("1/2/3/4",3),sigma = c(5,20,100))
mds_list = lapply(sim1[1:3], function(x){
  dist = dist2(x,x)
  mds = cmdscale(dist,2)
  return(mds)
})
g_sim1 = as_tibble(list.rbind(mds_list)) %>% 
  mutate(data_type = rep(c(1,2,3), each = 200),
         true = rep(sim1$truelabel, 3)) %>% 
  ggplot(aes(x = V1, y = V2, color = factor(true)))+
  geom_point()+
  facet_grid(data_type~., scale = "free")

g_sim1
```

Note that in this setting, the distance between 4 clusters are different. Denoting distance between cluster 1 and 2 as $d_{12}'$, then we have $d_{12}'=d_{13}' = d_{24}'<d_{14}' = d_{23}'<d_{34}'$

2) different types of data imply different cluster information

As mentioned above, in this setting, we assume 3 data types can separate 4 different subtypes, but different data types imply different clustering info. 3 data types separate AB/CD, AC/BD, AD/BC separately, with equall $(-\mu_1,\mu_1)$ as mean of distribution uniformly. 

Note that this is a completely balanced situation where all 4 subtypes are equally distant to each other.


```{r}
source("./code/functions/simulation_function.R")
sim2 = simulation_3(data_divide = c("12/34", "13/24","14/23"))
mds_list2 = lapply(sim2[1:3], function(x){
  dist = dist2(x,x)
  mds = cmdscale(dist,2)
  return(mds)
})
g_sim2= as_tibble(list.rbind(mds_list2)) %>% 
  mutate(data_type = rep(c(1,2,3), each = 200),
         true = rep(sim1$truelabel, 3)) %>% 
  ggplot(aes(x = V1, y = V2, color = factor(true)))+
  geom_point()+
  facet_grid(data_type~., scale = "free")

g_sim2
```




# the weight optimization

## intro the question 

From 50 running, we see that the mean weight for all 3 data types are balanced, but when looking at one-time output, there is always a dominant data, while the others contribute trivial. We need to figure out why this is the case.

```{r}
# table_balance = NULL
# for(i in 1:10){
#   tmp = simulation_3(size = 150,sub_ratio = rep(1/3,3), eff_size = c(1,1,1), data_divide =
#                        c("1/23","12/3","2/13"), sigma = 4)
#   res = simulation_verify_3data(K =3, tmp,tmp$truelabel)
#   table_balance = rbind(table_balance, res)
# }

load("./data/table_balance_3d3g.Rdata")
## show the direct output of weights for sym_balanced setting
table_balance[1:20,]%>% select(contains("weight"))
## the average of weight for sym_balanced setting
table_balance %>% select(contains("weight") | contains("nmi")) %>% summarise_all(mean)
```


## Derivation of weight

When fixing the similarity matrix

![derivation of weight](/Users/miaoyuqi/研究/Shuang project/multiomics-SIMLR/analysis/derivation_of_weights.jpeg)



From the derivation, we can see that the weight for a certain kernel depends on 2 parts:

* the sum of multiplication of distance and similarity matrix
* the hyperparameter $\rho$


### the influence of rho

Simulation of different scenario to see the effect of U

Scenario:
* eff_size: c(5,5),c(3,7)
* sub_ratio: rep(1/3,3),c(0.3,0.4,0.3)

Under every scenario, we test U = c(20,30,40,50) separately

We will focus
* the sum of weight for first data type 
* the final performance.

```{r}
files = list.files("./data/simulation_rho/")
table_rho = NULL
for(i in 1:length(files)){
  table_cur = readRDS(paste("./data/simulation_rho/",files[i],sep = ""))
  table_rho = rbind(table_rho, table_cur)
}
## show the mean performance
table_rho_mean = table_rho %>% 
  group_by(scenario,cluster) %>% 
  summarise_all(mean) %>% 
  mutate(nmi_improve = nmi_SIMLR-nmi_SNF) %>% 
  arrange(desc(nmi_improve))

table_rho_mean%>% 
  separate(scenario, into = c("sub_eff","U"), sep = "_(?=[^_]+$)") %>%
  pivot_longer(
    nmi_SNF:nmi_SIMLR,
    names_to = "methods",
    values_to = "value"
  ) %>% 
  ggplot(aes(x = U,y = value,color = methods)) +
  geom_boxplot()+
  facet_grid(sub_eff~.)

## show the weight change among U
table_rho %>% 
  separate(scenario, into = c("sub_eff","U"), sep = "_(?=[^_]+$)") %>%
  ggplot(aes(x = U,y = data1_weight,color = sub_eff)) +
  geom_boxplot()
  
```

From the simulation, we can see, when the variance is not extremely large, the performance of SIMLR will be relative stable. Only by changing weight, the performance of SIMLR can't show greater improve than SNF

Here, we can see the parameter $\rho$ (Or in this algorithm U) has a dominant role to control the variation of weights between kernels and data types. In this algorithm, the U is set to be 20 for all situation, which may not be proper for all situation. 

## Effect of SIMLR 
-- see the change of integrated similarity matrix

```{r}
## Data simulation
set.seed(34)
sim_eff37 = simulation_2(eff_size = c(3,7),sigma = c(18,(49*2)))
## show the data
par(mfrow = c(1,1))
### data 2 has larger signals but also larger var
plot(1:150, rowMeans(sim_eff37$data2[,1:50]),col = "red") 
### data1 has weaker signals also smaller var
points(1:150, rowMeans(sim_eff37$data1[,1:50]))
```



```{r}
## normalize data
data1 = standardNormalization(sim_eff37$data1)
data2 = standardNormalization(sim_eff37$data2)
truelabel = sim_eff37$truelabel
## fit SIMLR and get optimal weights separately
res1 = SIMLR(t(data1),c = 2)
res2 = SIMLR(t(data2),c = 2)
opt_w1 = res1$alphaK
opt_w2 = res2$alphaK
weight_list = list(opt_w1 = opt_w1, opt_w2 = opt_w2)

## get the combined multiple kernel separately
mk1 = multiple.kernel(data1,cores.ratio = 0.5)
mk2 = multiple.kernel(data2,cores.ratio = 0.5)
D_kernels = list()
for(i in 1:110){
  if(i<=55){
    D_kernels[[i]] = mk1[[i]]
  }else if (i<=110){
    D_kernels[[i]] = mk2[[i-55]]
  }
}
```


 

### What is the difference btw initial S_0 and S after SIMLR optimization?

```{r}

## we fisrt see the effect of averaged S_ij
### calculate the initial averaged S_ij
sum_kernels = matrix(0,150,150)
for(i in 1:length(D_kernels)){
  sum_kernels = sum_kernels+as.matrix(D_kernels[[i]])
}
s0 =  sum_kernels/length(D_kernels)
s0 = max(s0) - s0
cluster_s0 = spectralClustering(s0,3)
heatmap(s0, Rowv = NA, Colv = NA, scale = "column")


## Now we see the similarity matrix resulting from SIMLR
source("./code/functions/SIMLR_multi.R")
sim_res = SIMLR_multi(D_Kernels = D_kernels, c = 3)
s_res = sim_res$S
heatmap(s_res, Rowv = NA, Colv = NA, scale = "column")
compare(sim_res$y$cluster,rep(c(1,2,3), each = 50), method = "nmi")
compare(cluster_s0,rep(c(1,2,3), each = 50), method = "nmi")

```

As shown here, the multiplication sum has trivial difference for the initial $S_0$, but after the optimization by SIMLR, the difference btw 2 multiplication_sum become larger.

*Is this right? What is the role of S here? should the data be far away?*




# Possible problem and innovation of SIMLR

## Problem of SIMLR in this simulation

### Different clusters has different contribution requirment from data

#### Data simulation
```{r}
set.seed(234)
## get 2 balanced data type
library(tidyverse)
source("./code/functions/simulation_function.R")
set.seed(111)
sim = simulation_2(size = 150, sub_ratio = rep(1/3,3), eff_size = c(5,5), sigma = c(100,100),data_divide = c("1/23","12/3"),dist = c("normal","normal"))
```

```{r,,eval = F}
## separately fit the similarity matrix
res1 = SIMLR(t(sim$data1),2)
res2 = SIMLR(t(sim$data2),2)

## show the similarity difference
dist_s = dist2(res1$S, res2$S)
plot(density(dist_s))

plot(density(diag(dist_s)))

## show the dissimilar person within 2 similarity matrix
## use the last 25 quantiles of diagnal similarity
dissim = (1:150)[diag(dist_s) > quantile(diag(dist_s))[4]]

```



## Possible solutions

### the order of similarity in separate SIMLR results
Since the 2 data are seeparating different clusters, *the order of similarity* btw pairs should reflect some information:

For a given pair:
* if 2 data both contain compliant info, i.e., both cluster them into 1, or both not, then th order should be similar
* If 2 data contain different info, i.e., one data clustered the pair into 1 cluter, the other data clustered them separately, then the order should show difference.

Here we use a large variance balance case to verify the guess
```{r,eval = F}
library(tidyverse)
source("./code/functions/simulation_function.R")
set.seed(111)
sim = simulation_2(size = 150, sub_ratio = rep(1/3,3), eff_size = c(5,5), sigma = c(100,100),data_divide = c("1/23","12/3"),dist = c("normal","normal"))

## see the distribution of 2 data types
par(mfrow = c(2,1))
plot(1:150, rowMeans(sim$data1[,1:50]),main = "data1")
plot(1:150, rowMeans(sim$data2[,1:50]),main = "data2")

## Do SIMLE separately
res1 = SIMLR(t(sim$data1),2)
res2 = SIMLR(t(sim$data2),2)

## see the difference of order of 2 similarity matrix
# sim_ord1 = sort(res1$S,index.return = T)$ix
# sim_ord2 = sort(res2$S,index.return = T)$ix
# dif_ord = match(sim_ord1,sim_ord2)-(1:length(sim_ord1))
# par(mfrow = c(1,1))
# plot(density(dif_ord)) 

# pair_to_Sij plot for both data
par(mfrow = c(2,1))
plot(1:150^2,as.vector(res1$S))
plot(1:150^2,res2$S)

```



#### What if we directly choose the largest distance in 2 data for every pair?
```{r,eval = F}
## first calculate the weighted distance
mk1 = multiple.kernel(sim$data1,cores.ratio = 0.5)
mk2 = multiple.kernel(sim$data2,cores.ratio = 0.5)

opt_dist1 = matrix(0,150,150)
for(i in 1:55){
  opt_dist1 = opt_dist1 + res1$alphaK[i]*mk1[[i]]
}

opt_dist2 = matrix(0,150,150)
for(i in 1:55){
  opt_dist2 = opt_dist2 + res2$alphaK[i]*mk2[[i]]
}
## then, for the similar ordered pairs, we use the min distance, for the dissimilar pairs, we use the max distance 
c1 = 1000
c2 = 1000
distX = matrix(0,150,150)
for(i in 1:150^2){
  if(dif_ord[i]<-c1 |dif_ord[i] > c2){
    distX[i] = max(opt_dist1[i],opt_dist2[i])
  }else{
    distX[i] = min(opt_dist1[i],opt_dist2[i])
  }
}
heatmap(distX,Rowv = NA, Colv = NA, scale = "column")


## then we should change the SIMLR process, we don't need weight and multiple kernels, we only need the process of rank constraint and max multiplication_sum
source("./code/functions/SIMLR_no_weights.R")
res_sep = SIMLR_no_weight(distX,c = 3)
compare(res_sep$y$cluster, sim$truelabel,method = "nmi")
```










### [TO THINK] Add freedom of weight -- specific weight for every cluster

New term in the objective function:
$$\min_{w} \sum_{i,j,t,l}w_{tl}D_{l,i,j}S_{ij}I\{sub_{ij}\in C_t\}$$
t is for different cluster  





































